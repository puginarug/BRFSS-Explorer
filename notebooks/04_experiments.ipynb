{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m5-0",
   "metadata": {},
   "source": [
    "# ScreenMind — Milestone 5: Experiment Tracking with Weights & Biases\n",
    "\n",
    "**Goal:** Train 3 model variants, log every run to W&B, and compare them on a live dashboard.\n",
    "\n",
    "**Why W&B?**  \n",
    "In Milestone 4 we trained one model and printed results to the screen.  That works for one run,\n",
    "but as soon as you want to compare architectures or tune hyperparameters you need a systematic record.\n",
    "W&B stores every metric, every hyperparameter, and every artifact from every run in a searchable\n",
    "cloud dashboard — so you can see at a glance which config won and why.\n",
    "\n",
    "**The 3 variants we will compare:**\n",
    "\n",
    "| Variant | Architecture | Reg loss | Key idea |\n",
    "|---------|-------------|----------|----------|\n",
    "| A — Baseline | 128 → 64 | MSE | Exactly what we trained in M4 |\n",
    "| B — Wider | 256 → 128 → 64 | MSE | More parameters → can it learn richer patterns? |\n",
    "| C — Weighted loss | 128 → 64 | WeightedMSE | Penalise errors on high-day cases 2× harder |\n",
    "\n",
    "Each variant trains **both** a classifier and a regressor.  All 6 runs appear in the same W&B project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-1",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ed31db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n"
     ]
    }
   ],
   "source": [
    "print(\"check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "m5-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src.data.preprocessing import load_processed\n",
    "from src.models.mlp import MLP\n",
    "from src.training.trainer import (\n",
    "    make_loaders, make_criterion, train,\n",
    "    evaluate_clf, evaluate_reg,\n",
    ")\n",
    "\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "MODELS_DIR    = '../data/models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "data = load_processed(PROCESSED_DIR)\n",
    "print('Data loaded.')\n",
    "print(f'Device: {\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-3",
   "metadata": {},
   "source": [
    "## 2. How W&B Works\n",
    "\n",
    "The W&B workflow has three steps:\n",
    "\n",
    "```\n",
    "wandb.init(project=..., config=...)   # 1. Open a new run, store hyperparams\n",
    "    wandb.run.log({...})              # 2. Stream metrics each epoch\n",
    "    wandb.run.summary.update({...})   # 3. Record final test metrics\n",
    "wandb.finish()                        # 4. Close the run\n",
    "```\n",
    "\n",
    "**`config`** is a plain dict of all hyperparameters (architecture, lr, loss, etc.).\n",
    "W&B stores it alongside the metrics so you can filter runs by config in the dashboard.\n",
    "\n",
    "**`log`** is called once per epoch and streams numbers to the cloud in real time.\n",
    "This is what draws the live loss curves on the dashboard.\n",
    "\n",
    "**`summary`** is for final / aggregate metrics (test AUC, MAE, etc.) — values that\n",
    "only exist after training is complete.\n",
    "\n",
    "Our `train()` function already accepts `wandb_run=` — we just pass `wandb.run` to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-4",
   "metadata": {},
   "source": [
    "## 3. Define the 3 Variant Configs\n",
    "\n",
    "We express each variant as a plain Python dict.  This dict becomes the W&B config\n",
    "and is also used to construct the model and criterion — one source of truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "m5-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variant_a_baseline              hidden=[128, 64]  params=10,753  reg_loss=reg\n",
      "variant_b_wider                 hidden=[256, 128, 64]  params=46,209  reg_loss=reg\n",
      "variant_c_weighted_loss         hidden=[128, 64]  params=10,753  reg_loss=reg_weighted\n"
     ]
    }
   ],
   "source": [
    "VARIANTS = [\n",
    "    {\n",
    "        \"name\":        \"variant_a_baseline\",\n",
    "        \"hidden_dims\": [128, 64],\n",
    "        \"dropout\":     0.3,\n",
    "        \"lr\":          1e-3,\n",
    "        \"reg_loss\":    \"reg\",          # plain MSELoss\n",
    "        \"batch_size\":  512,\n",
    "        \"max_epochs\":  100,\n",
    "        \"patience\":    10,\n",
    "        \"description\": \"Baseline from Milestone 4 — replicated for fair comparison\",\n",
    "    },\n",
    "    {\n",
    "        \"name\":        \"variant_b_wider\",\n",
    "        \"hidden_dims\": [256, 128, 64],  # extra layer, more parameters\n",
    "        \"dropout\":     0.3,\n",
    "        \"lr\":          1e-3,\n",
    "        \"reg_loss\":    \"reg\",\n",
    "        \"batch_size\":  512,\n",
    "        \"max_epochs\":  100,\n",
    "        \"patience\":    10,\n",
    "        \"description\": \"Wider MLP: 3 hidden layers instead of 2\",\n",
    "    },\n",
    "    {\n",
    "        \"name\":        \"variant_c_weighted_loss\",\n",
    "        \"hidden_dims\": [128, 64],\n",
    "        \"dropout\":     0.3,\n",
    "        \"lr\":          1e-3,\n",
    "        \"reg_loss\":    \"reg_weighted\",  # WeightedMSELoss\n",
    "        \"batch_size\":  512,\n",
    "        \"max_epochs\":  100,\n",
    "        \"patience\":    10,\n",
    "        \"description\": \"Same architecture as A but WeightedMSE penalises high-day errors 2x\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for v in VARIANTS:\n",
    "    n_params = MLP(input_dim=15, hidden_dims=v['hidden_dims'], dropout=v['dropout']).count_parameters()\n",
    "    print(f\"{v['name']:<30}  hidden={v['hidden_dims']}  params={n_params:,}  reg_loss={v['reg_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-6",
   "metadata": {},
   "source": [
    "## 4. W&B Login\n",
    "\n",
    "Run this cell once to authenticate.  \n",
    "It will open a browser tab asking you to paste your API key (free account at wandb.ai).\n",
    "\n",
    "After the first login, W&B caches your key — you won't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "m5-7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()  # opens browser / prompts for API key the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-8",
   "metadata": {},
   "source": [
    "## 5. Run the Experiments\n",
    "\n",
    "The loop below:\n",
    "1. Opens a W&B run for each (variant, task) combination\n",
    "2. Trains the model, streaming per-epoch losses to W&B\n",
    "3. Evaluates on the test set and writes final metrics to the W&B summary\n",
    "4. Closes the run\n",
    "\n",
    "**6 runs total** (3 variants × 2 tasks: clf + reg).  \n",
    "While it trains, open your [W&B project dashboard](https://wandb.ai) to watch the curves live.\n",
    "\n",
    "⏱️ On CPU this takes ~15-25 minutes total — each variant trains at most 100 epochs, with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "m5-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run: variant_a_baseline_clf\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\zinge\\coding_projects\\ScreenTimeML\\notebooks\\wandb\\run-20260226_103859-17guxcka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zinger25/screenmind/runs/17guxcka' target=\"_blank\">variant_a_baseline_clf</a></strong> to <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zinger25/screenmind/runs/17guxcka' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/17guxcka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1       0.86401       0.83318   ✓\n",
      "     2       0.84638       0.83062   ✓\n",
      "     3       0.84331       0.82943   ✓\n",
      "     4       0.84115       0.82890   ✓\n",
      "     5       0.83989       0.82988  \n",
      "     6       0.83952       0.82975  \n",
      "     7       0.83844       0.82843   ✓\n",
      "     8       0.83818       0.82820   ✓\n",
      "     9       0.83768       0.82789   ✓\n",
      "    10       0.83697       0.82834  \n",
      "    11       0.83658       0.82775   ✓\n",
      "    12       0.83666       0.82723   ✓\n",
      "    13       0.83663       0.82771  \n",
      "    14       0.83622       0.82792  \n",
      "    15       0.83512       0.82793  \n",
      "    16       0.83560       0.82777  \n",
      "    17       0.83500       0.82744  \n",
      "    18       0.83566       0.82763  \n",
      "    19       0.83502       0.82745  \n",
      "    20       0.83434       0.82711   ✓\n",
      "    21       0.83505       0.82705   ✓\n",
      "    22       0.83483       0.82708  \n",
      "    23       0.83509       0.82720  \n",
      "    24       0.83379       0.82723  \n",
      "    25       0.83373       0.82711  \n",
      "    26       0.83357       0.82664   ✓\n",
      "    27       0.83330       0.82717  \n",
      "    28       0.83256       0.82746  \n",
      "    29       0.83396       0.82724  \n",
      "    30       0.83373       0.82703  \n",
      "    31       0.83325       0.82668  \n",
      "    32       0.83282       0.82662   ✓\n",
      "    33       0.83280       0.82659   ✓\n",
      "    34       0.83266       0.82647   ✓\n",
      "    35       0.83345       0.82663  \n",
      "    36       0.83313       0.82643   ✓\n",
      "    37       0.83253       0.82654  \n",
      "    38       0.83305       0.82734  \n",
      "    39       0.83255       0.82681  \n",
      "    40       0.83260       0.82664  \n",
      "    41       0.83252       0.82672  \n",
      "    42       0.83138       0.82595   ✓\n",
      "    43       0.83207       0.82669  \n",
      "    44       0.83180       0.82645  \n",
      "    45       0.83174       0.82620  \n",
      "    46       0.83121       0.82698  \n",
      "    47       0.83117       0.82671  \n",
      "    48       0.83199       0.82710  \n",
      "    49       0.83086       0.82751  \n",
      "    50       0.83117       0.82658  \n",
      "    51       0.83218       0.82800  \n",
      "    52       0.83058       0.82731  \n",
      "\n",
      "Early stopping at epoch 52 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 0.82595  →  saved to ..\\data\\models\\variant_a_baseline_clf_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC=0.8561  F1=0.4875  Recall=0.7746  Precision=0.3557\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▄▅▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.78456</td></tr><tr><td>best_val_loss</td><td>0.82595</td></tr><tr><td>epoch</td><td>52</td></tr><tr><td>epochs_trained</td><td>52</td></tr><tr><td>f1</td><td>0.48753</td></tr><tr><td>n_samples</td><td>67428</td></tr><tr><td>precision</td><td>0.35571</td></tr><tr><td>recall</td><td>0.77458</td></tr><tr><td>roc_auc</td><td>0.85613</td></tr><tr><td>train_loss</td><td>0.83058</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">variant_a_baseline_clf</strong> at: <a href='https://wandb.ai/zinger25/screenmind/runs/17guxcka' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/17guxcka</a><br> View project at: <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260226_103859-17guxcka\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run: variant_a_baseline_reg\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\zinge\\coding_projects\\ScreenTimeML\\notebooks\\wandb\\run-20260226_104521-prxangvc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zinger25/screenmind/runs/prxangvc' target=\"_blank\">variant_a_baseline_reg</a></strong> to <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zinger25/screenmind/runs/prxangvc' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/prxangvc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1      50.49163      46.05582   ✓\n",
      "     2      47.18585      45.90093   ✓\n",
      "     3      46.97400      45.79371   ✓\n",
      "     4      46.88458      45.69613   ✓\n",
      "     5      46.74306      45.71967  \n",
      "     6      46.69943      45.66703   ✓\n",
      "     7      46.66086      45.61792   ✓\n",
      "     8      46.58563      45.71859  \n",
      "     9      46.60715      45.64407  \n",
      "    10      46.56724      45.62900  \n",
      "    11      46.52922      45.60190   ✓\n",
      "    12      46.50346      45.59019   ✓\n",
      "    13      46.52180      45.63809  \n",
      "    14      46.48891      45.59873  \n",
      "    15      46.46805      45.59681  \n",
      "    16      46.42696      45.57326   ✓\n",
      "    17      46.44442      45.70117  \n",
      "    18      46.45649      45.60152  \n",
      "    19      46.41776      45.56942   ✓\n",
      "    20      46.38597      45.54929   ✓\n",
      "    21      46.36916      45.58523  \n",
      "    22      46.36635      45.57831  \n",
      "    23      46.34396      45.52391   ✓\n",
      "    24      46.27873      45.58432  \n",
      "    25      46.32414      45.58464  \n",
      "    26      46.32886      45.52077   ✓\n",
      "    27      46.33398      45.53185  \n",
      "    28      46.30865      45.53387  \n",
      "    29      46.36406      45.55177  \n",
      "    30      46.32322      45.60483  \n",
      "    31      46.34542      45.62014  \n",
      "    32      46.23966      45.54173  \n",
      "    33      46.23908      45.55363  \n",
      "    34      46.24952      45.51809   ✓\n",
      "    35      46.23632      45.55592  \n",
      "    36      46.31567      45.56401  \n",
      "    37      46.27321      45.53282  \n",
      "    38      46.20954      45.51613   ✓\n",
      "    39      46.21799      45.53869  \n",
      "    40      46.30852      45.52408  \n",
      "    41      46.28016      45.52891  \n",
      "    42      46.27197      45.53274  \n",
      "    43      46.25371      45.56885  \n",
      "    44      46.20686      45.54128  \n",
      "    45      46.25288      45.50508   ✓\n",
      "    46      46.22819      45.54195  \n",
      "    47      46.26900      45.51564  \n",
      "    48      46.22834      45.50454   ✓\n",
      "    49      46.14526      45.65493  \n",
      "    50      46.21649      45.53960  \n",
      "    51      46.23125      45.52738  \n",
      "    52      46.26968      45.50525  \n",
      "    53      46.16188      45.52730  \n",
      "    54      46.20956      45.57211  \n",
      "    55      46.20700      45.53109  \n",
      "    56      46.19485      45.54301  \n",
      "    57      46.16024      45.53310  \n",
      "    58      46.18598      45.49402   ✓\n",
      "    59      46.22438      45.54249  \n",
      "    60      46.15782      45.50488  \n",
      "    61      46.16538      45.53812  \n",
      "    62      46.18077      45.52603  \n",
      "    63      46.20635      45.51544  \n",
      "    64      46.16722      45.50853  \n",
      "    65      46.17658      45.56933  \n",
      "    66      46.14172      45.51711  \n",
      "    67      46.18635      45.52397  \n",
      "    68      46.11783      45.49030   ✓\n",
      "    69      46.16100      45.52072  \n",
      "    70      46.16692      45.50844  \n",
      "    71      46.11739      45.53388  \n",
      "    72      46.14313      45.51720  \n",
      "    73      46.13695      45.51769  \n",
      "    74      46.14540      45.51596  \n",
      "    75      46.14805      45.56592  \n",
      "    76      46.12465      45.50306  \n",
      "    77      46.15997      45.52058  \n",
      "    78      46.17944      45.65292  \n",
      "\n",
      "Early stopping at epoch 78 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 45.49030  →  saved to ..\\data\\models\\variant_a_baseline_reg_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE=4.155  RMSE=6.720  R²=0.3521\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>train_loss</td><td>██▇▆▆▆▅▅▄▄▄▄▄▄▄▃▂▄▂▂▃▃▃▂▃▂▁▂▂▂▂▁▂▁▂▁▁▁▁▂</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▃▂▂▁▂▁▁▂▂▁▁▂▁▁▁▁▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>45.4903</td></tr><tr><td>epoch</td><td>78</td></tr><tr><td>epochs_trained</td><td>78</td></tr><tr><td>mae</td><td>4.15545</td></tr><tr><td>n_samples</td><td>67428</td></tr><tr><td>r2</td><td>0.35212</td></tr><tr><td>rmse</td><td>6.72001</td></tr><tr><td>train_loss</td><td>46.17944</td></tr><tr><td>val_loss</td><td>45.65292</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">variant_a_baseline_reg</strong> at: <a href='https://wandb.ai/zinger25/screenmind/runs/prxangvc' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/prxangvc</a><br> View project at: <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260226_104521-prxangvc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run: variant_b_wider_clf\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\zinge\\coding_projects\\ScreenTimeML\\notebooks\\wandb\\run-20260226_105648-s79e4bdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zinger25/screenmind/runs/s79e4bdb' target=\"_blank\">variant_b_wider_clf</a></strong> to <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zinger25/screenmind/runs/s79e4bdb' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/s79e4bdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1       0.85767       0.83103   ✓\n",
      "     2       0.84537       0.82989   ✓\n",
      "     3       0.84214       0.82959   ✓\n",
      "     4       0.83985       0.82853   ✓\n",
      "     5       0.83958       0.82785   ✓\n",
      "     6       0.83845       0.82787  \n",
      "     7       0.83804       0.82904  \n",
      "     8       0.83739       0.82805  \n",
      "     9       0.83629       0.82694   ✓\n",
      "    10       0.83650       0.82683   ✓\n",
      "    11       0.83500       0.82682   ✓\n",
      "    12       0.83588       0.82716  \n",
      "    13       0.83519       0.82617   ✓\n",
      "    14       0.83438       0.82652  \n",
      "    15       0.83419       0.82776  \n",
      "    16       0.83378       0.82612   ✓\n",
      "    17       0.83370       0.82674  \n",
      "    18       0.83356       0.82620  \n",
      "    19       0.83285       0.82698  \n",
      "    20       0.83223       0.82538   ✓\n",
      "    21       0.83268       0.82612  \n",
      "    22       0.83267       0.82566  \n",
      "    23       0.83208       0.82653  \n",
      "    24       0.83202       0.82602  \n",
      "    25       0.83098       0.82691  \n",
      "    26       0.83063       0.82629  \n",
      "    27       0.83017       0.82585  \n",
      "    28       0.83103       0.82644  \n",
      "    29       0.83046       0.82649  \n",
      "    30       0.83003       0.82630  \n",
      "\n",
      "Early stopping at epoch 30 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 0.82538  →  saved to ..\\data\\models\\variant_b_wider_clf_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC=0.8564  F1=0.4847  Recall=0.7778  Precision=0.3520\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▆▄▃▃▃▃▂▂▄▂▃▂▃▁▂▁▂▂▃▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.78119</td></tr><tr><td>best_val_loss</td><td>0.82538</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>epochs_trained</td><td>30</td></tr><tr><td>f1</td><td>0.4847</td></tr><tr><td>n_samples</td><td>67428</td></tr><tr><td>precision</td><td>0.35204</td></tr><tr><td>recall</td><td>0.77783</td></tr><tr><td>roc_auc</td><td>0.85637</td></tr><tr><td>train_loss</td><td>0.83003</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">variant_b_wider_clf</strong> at: <a href='https://wandb.ai/zinger25/screenmind/runs/s79e4bdb' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/s79e4bdb</a><br> View project at: <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260226_105648-s79e4bdb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run: variant_b_wider_reg\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\zinge\\coding_projects\\ScreenTimeML\\notebooks\\wandb\\run-20260226_110233-5t9mfvf1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zinger25/screenmind/runs/5t9mfvf1' target=\"_blank\">variant_b_wider_reg</a></strong> to <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zinger25/screenmind/runs/5t9mfvf1' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/5t9mfvf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1      49.89957      45.75487   ✓\n",
      "     2      47.17452      45.72248   ✓\n",
      "     3      47.11304      45.67145   ✓\n",
      "     4      46.91575      45.64142   ✓\n",
      "     5      46.82398      45.68896  \n",
      "     6      46.72905      45.58446   ✓\n",
      "     7      46.66932      45.54983   ✓\n",
      "     8      46.64963      45.58336  \n",
      "     9      46.64902      45.57214  \n",
      "    10      46.56154      45.56769  \n",
      "    11      46.58170      45.52601   ✓\n",
      "    12      46.46164      45.55773  \n",
      "    13      46.44419      45.52750  \n",
      "    14      46.44137      45.53166  \n",
      "    15      46.36834      45.53254  \n",
      "    16      46.30252      45.50596   ✓\n",
      "    17      46.35646      45.53694  \n",
      "    18      46.38856      45.53119  \n",
      "    19      46.36033      45.52502  \n",
      "    20      46.25116      45.56150  \n",
      "    21      46.31244      45.53346  \n",
      "    22      46.24528      45.53128  \n",
      "    23      46.16934      45.51905  \n",
      "    24      46.23029      45.52864  \n",
      "    25      46.19049      45.54527  \n",
      "    26      46.18983      45.55672  \n",
      "\n",
      "Early stopping at epoch 26 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 45.50596  →  saved to ..\\data\\models\\variant_b_wider_reg_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE=4.187  RMSE=6.725  R²=0.3512\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▆▃▂▃▃▃▂▂▂▂▂▁▂▂▂▃▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>45.50596</td></tr><tr><td>epoch</td><td>26</td></tr><tr><td>epochs_trained</td><td>26</td></tr><tr><td>mae</td><td>4.1865</td></tr><tr><td>n_samples</td><td>67428</td></tr><tr><td>r2</td><td>0.35123</td></tr><tr><td>rmse</td><td>6.72463</td></tr><tr><td>train_loss</td><td>46.18983</td></tr><tr><td>val_loss</td><td>45.55672</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">variant_b_wider_reg</strong> at: <a href='https://wandb.ai/zinger25/screenmind/runs/5t9mfvf1' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/5t9mfvf1</a><br> View project at: <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260226_110233-5t9mfvf1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run: variant_c_weighted_loss_clf\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\zinge\\coding_projects\\ScreenTimeML\\notebooks\\wandb\\run-20260226_110740-5n9093x7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zinger25/screenmind/runs/5n9093x7' target=\"_blank\">variant_c_weighted_loss_clf</a></strong> to <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zinger25/screenmind/runs/5n9093x7' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/5n9093x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1       0.86238       0.83131   ✓\n",
      "     2       0.84539       0.82999   ✓\n",
      "     3       0.84235       0.82879   ✓\n",
      "     4       0.84071       0.82839   ✓\n",
      "     5       0.83974       0.82901  \n",
      "     6       0.83850       0.82818   ✓\n",
      "     7       0.83880       0.82889  \n",
      "     8       0.83728       0.82787   ✓\n",
      "     9       0.83724       0.82801  \n",
      "    10       0.83710       0.82812  \n",
      "    11       0.83709       0.82745   ✓\n",
      "    12       0.83605       0.82764  \n",
      "    13       0.83562       0.82737   ✓\n",
      "    14       0.83566       0.82759  \n",
      "    15       0.83538       0.82670   ✓\n",
      "    16       0.83485       0.82690  \n",
      "    17       0.83542       0.82700  \n",
      "    18       0.83545       0.82715  \n",
      "    19       0.83490       0.82700  \n",
      "    20       0.83492       0.82668   ✓\n",
      "    21       0.83480       0.82727  \n",
      "    22       0.83477       0.82717  \n",
      "    23       0.83452       0.82706  \n",
      "    24       0.83418       0.82701  \n",
      "    25       0.83329       0.82652   ✓\n",
      "    26       0.83375       0.82736  \n",
      "    27       0.83343       0.82633   ✓\n",
      "    28       0.83290       0.82628   ✓\n",
      "    29       0.83386       0.82587   ✓\n",
      "    30       0.83311       0.82659  \n",
      "    31       0.83320       0.82711  \n",
      "    32       0.83265       0.82608  \n",
      "    33       0.83266       0.82568   ✓\n",
      "    34       0.83242       0.82596  \n",
      "    35       0.83245       0.82615  \n",
      "    36       0.83289       0.82640  \n",
      "    37       0.83232       0.82587  \n",
      "    38       0.83248       0.82590  \n",
      "    39       0.83226       0.82583  \n",
      "    40       0.83225       0.82628  \n",
      "    41       0.83191       0.82591  \n",
      "    42       0.83203       0.82621  \n",
      "    43       0.83200       0.82577  \n",
      "\n",
      "Early stopping at epoch 43 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 0.82568  →  saved to ..\\data\\models\\variant_c_weighted_loss_clf_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC=0.8563  F1=0.4864  Recall=0.7751  Precision=0.3544\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▄▅▄▅▄▄▄▃▃▃▂▃▃▃▃▂▃▃▃▃▂▃▂▁▂▃▂▁▁▂▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.78341</td></tr><tr><td>best_val_loss</td><td>0.82568</td></tr><tr><td>epoch</td><td>43</td></tr><tr><td>epochs_trained</td><td>43</td></tr><tr><td>f1</td><td>0.48639</td></tr><tr><td>n_samples</td><td>67428</td></tr><tr><td>precision</td><td>0.35438</td></tr><tr><td>recall</td><td>0.77514</td></tr><tr><td>roc_auc</td><td>0.8563</td></tr><tr><td>train_loss</td><td>0.832</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">variant_c_weighted_loss_clf</strong> at: <a href='https://wandb.ai/zinger25/screenmind/runs/5n9093x7' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/5n9093x7</a><br> View project at: <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260226_110740-5n9093x7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting run: variant_c_weighted_loss_reg\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\zinge\\coding_projects\\ScreenTimeML\\notebooks\\wandb\\run-20260226_111243-a8ypbri4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zinger25/screenmind/runs/a8ypbri4' target=\"_blank\">variant_c_weighted_loss_reg</a></strong> to <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zinger25/screenmind/runs/a8ypbri4' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/a8ypbri4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1      83.80939      72.99685   ✓\n",
      "     2      75.12024      72.79371   ✓\n",
      "     3      74.86382      72.58828   ✓\n",
      "     4      74.59381      72.54819   ✓\n",
      "     5      74.45407      72.78791  \n",
      "     6      74.30140      72.56605  \n",
      "     7      74.15848      72.46526   ✓\n",
      "     8      74.02201      72.47620  \n",
      "     9      74.07983      72.27989   ✓\n",
      "    10      74.10324      72.38026  \n",
      "    11      73.94366      72.32950  \n",
      "    12      73.82160      72.33438  \n",
      "    13      73.91891      72.36322  \n",
      "    14      73.90058      72.25301   ✓\n",
      "    15      73.83213      72.30286  \n",
      "    16      73.77368      72.25423  \n",
      "    17      73.76725      72.22230   ✓\n",
      "    18      73.82047      72.46912  \n",
      "    19      73.75924      72.37707  \n",
      "    20      73.74145      72.23087  \n",
      "    21      73.71404      72.33050  \n",
      "    22      73.68846      72.34884  \n",
      "    23      73.65688      72.21722   ✓\n",
      "    24      73.53658      72.24353  \n",
      "    25      73.63722      72.23109  \n",
      "    26      73.67636      72.24334  \n",
      "    27      73.53818      72.35068  \n",
      "    28      73.52486      72.24355  \n",
      "    29      73.56131      72.20893   ✓\n",
      "    30      73.49004      72.21925  \n",
      "    31      73.56829      72.26165  \n",
      "    32      73.50363      72.27511  \n",
      "    33      73.48482      72.20047   ✓\n",
      "    34      73.65685      72.24267  \n",
      "    35      73.62216      72.18989   ✓\n",
      "    36      73.49614      72.26231  \n",
      "    37      73.47146      72.21075  \n",
      "    38      73.58080      72.18144   ✓\n",
      "    39      73.50860      72.36077  \n",
      "    40      73.53845      72.27340  \n",
      "    41      73.44756      72.25045  \n",
      "    42      73.41846      72.26216  \n",
      "    43      73.46079      72.24564  \n",
      "    44      73.47784      72.25884  \n",
      "    45      73.40209      72.29105  \n",
      "    46      73.48400      72.21159  \n",
      "    47      73.45177      72.51869  \n",
      "    48      73.40874      72.26317  \n",
      "\n",
      "Early stopping at epoch 48 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 72.18144  →  saved to ..\\data\\models\\variant_c_weighted_loss_reg_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE=4.675  RMSE=6.879  R²=0.3212\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▄▆▃▄▂▃▂▃▂▂▂▁▃▁▂▂▁▁▂▂▂▁▂▂▁▂▁▁▁▃▂▂▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>72.18144</td></tr><tr><td>epoch</td><td>48</td></tr><tr><td>epochs_trained</td><td>48</td></tr><tr><td>mae</td><td>4.67487</td></tr><tr><td>n_samples</td><td>67428</td></tr><tr><td>r2</td><td>0.32119</td></tr><tr><td>rmse</td><td>6.87853</td></tr><tr><td>train_loss</td><td>73.40874</td></tr><tr><td>val_loss</td><td>72.26317</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">variant_c_weighted_loss_reg</strong> at: <a href='https://wandb.ai/zinger25/screenmind/runs/a8ypbri4' target=\"_blank\">https://wandb.ai/zinger25/screenmind/runs/a8ypbri4</a><br> View project at: <a href='https://wandb.ai/zinger25/screenmind' target=\"_blank\">https://wandb.ai/zinger25/screenmind</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260226_111243-a8ypbri4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All runs complete.\n"
     ]
    }
   ],
   "source": [
    "all_results = []   # collect test metrics across all runs for the comparison table\n",
    "\n",
    "for cfg in VARIANTS:\n",
    "    for task in ('clf', 'reg'):\n",
    "\n",
    "        run_name = f\"{cfg['name']}_{task}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting run: {run_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # ── Open a W&B run ────────────────────────────────────────────────────\n",
    "        # project: groups all runs together in one W&B project page\n",
    "        # name:    human-readable label for this run in the dashboard\n",
    "        # config:  stores all hyperparams so you can filter/sort by them later\n",
    "        run = wandb.init(\n",
    "            project=\"screenmind\",\n",
    "            name=run_name,\n",
    "            config={**cfg, \"task\": task, \"input_dim\": 15},\n",
    "            reinit=True,   # allow multiple inits in one notebook session\n",
    "        )\n",
    "\n",
    "        # ── Build model & loaders ─────────────────────────────────────────────\n",
    "        model = MLP(\n",
    "            input_dim=15,\n",
    "            hidden_dims=cfg['hidden_dims'],\n",
    "            dropout=cfg['dropout'],\n",
    "            task=task,\n",
    "        )\n",
    "\n",
    "        train_loader, val_loader, test_loader = make_loaders(\n",
    "            data, task=task, batch_size=cfg['batch_size']\n",
    "        )\n",
    "\n",
    "        # For the classifier always use 'clf'; for reg use whatever loss the variant specifies\n",
    "        criterion_task = 'clf' if task == 'clf' else cfg['reg_loss']\n",
    "        y_train = data['y_clf_train'] if task == 'clf' else data['y_reg_train']\n",
    "        criterion = make_criterion(criterion_task, y_train)\n",
    "\n",
    "        checkpoint_path = f'{MODELS_DIR}/{run_name}_best.pt'\n",
    "\n",
    "        # ── Train (losses stream to W&B each epoch via wandb_run=run) ─────────\n",
    "        history = train(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            lr=cfg['lr'],\n",
    "            max_epochs=cfg['max_epochs'],\n",
    "            patience=cfg['patience'],\n",
    "            wandb_run=run,\n",
    "        )\n",
    "\n",
    "        # ── Evaluate on test set ──────────────────────────────────────────────\n",
    "        model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "\n",
    "        if task == 'clf':\n",
    "            metrics = evaluate_clf(model, test_loader)\n",
    "            print(f\"  AUC={metrics['roc_auc']:.4f}  F1={metrics['f1']:.4f}  \"\n",
    "                  f\"Recall={metrics['recall']:.4f}  Precision={metrics['precision']:.4f}\")\n",
    "        else:\n",
    "            metrics = evaluate_reg(model, test_loader)\n",
    "            print(f\"  MAE={metrics['mae']:.3f}  RMSE={metrics['rmse']:.3f}  R²={metrics['r2']:.4f}\")\n",
    "\n",
    "        # ── Write final metrics to W&B summary (appears in the runs table) ────\n",
    "        # summary values show up as columns in the W&B project table,\n",
    "        # making it easy to sort runs by AUC or MAE at a glance.\n",
    "        run.summary.update(metrics)\n",
    "        run.summary[\"best_val_loss\"] = min(history['val_loss'])\n",
    "        run.summary[\"epochs_trained\"] = len(history['val_loss'])\n",
    "\n",
    "        all_results.append({\"run\": run_name, \"task\": task, **metrics})\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "print(\"\\n✓ All runs complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-10",
   "metadata": {},
   "source": [
    "## 6. Local Comparison Table\n",
    "\n",
    "W&B gives you an interactive dashboard, but it's also useful to see the summary locally.\n",
    "\n",
    "**How to read the classifier table:**  \n",
    "- Focus on **AUC** (overall discrimination) and **Recall** (did we catch high-risk patients?)  \n",
    "- F1 will always look modest with 13% positive rate — don't be misled by it\n",
    "\n",
    "**How to read the regressor table:**  \n",
    "- Focus on **MAE** (interpretable in days) and **R²** (how much variance explained)  \n",
    "- Variant C (WeightedMSE) may trade overall MAE for better accuracy on high-day cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "m5-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLASSIFIER RESULTS ===\n",
      "                        Run    AUC     F1  Recall  Precision  Accuracy\n",
      "        variant_b_wider_clf 0.8564 0.4847  0.7778     0.3520    0.7812\n",
      "variant_c_weighted_loss_clf 0.8563 0.4864  0.7751     0.3544    0.7834\n",
      "     variant_a_baseline_clf 0.8561 0.4875  0.7746     0.3557    0.7846\n",
      "\n",
      "=== REGRESSOR RESULTS ===\n",
      "                        Run    MAE   RMSE     R²\n",
      "     variant_a_baseline_reg 4.1554 6.7200 0.3521\n",
      "        variant_b_wider_reg 4.1865 6.7246 0.3512\n",
      "variant_c_weighted_loss_reg 4.6749 6.8785 0.3212\n",
      "\n",
      "Best classifier: variant_b_wider_clf  (AUC=0.8564)\n",
      "Best regressor:  variant_a_baseline_reg  (MAE=4.155)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clf_rows = [r for r in all_results if r['task'] == 'clf']\n",
    "reg_rows = [r for r in all_results if r['task'] == 'reg']\n",
    "\n",
    "print('=== CLASSIFIER RESULTS ===')\n",
    "clf_df = pd.DataFrame(clf_rows)[['run', 'roc_auc', 'f1', 'recall', 'precision', 'accuracy']]\n",
    "clf_df.columns = ['Run', 'AUC', 'F1', 'Recall', 'Precision', 'Accuracy']\n",
    "clf_df = clf_df.sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "print(clf_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "print()\n",
    "print('=== REGRESSOR RESULTS ===')\n",
    "reg_df = pd.DataFrame(reg_rows)[['run', 'mae', 'rmse', 'r2']]\n",
    "reg_df.columns = ['Run', 'MAE', 'RMSE', 'R²']\n",
    "reg_df = reg_df.sort_values('MAE').reset_index(drop=True)\n",
    "print(reg_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "print()\n",
    "best_clf = clf_df.iloc[0]\n",
    "best_reg = reg_df.iloc[0]\n",
    "print(f\"Best classifier: {best_clf['Run']}  (AUC={best_clf['AUC']:.4f})\")\n",
    "print(f\"Best regressor:  {best_reg['Run']}  (MAE={best_reg['MAE']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5-12",
   "metadata": {},
   "source": [
    "## 7. Milestone 5 Summary\n",
    "\n",
    "**What we did:**\n",
    "- Added `WeightedMSELoss` to `src/training/trainer.py` — a custom loss that gives 2× penalty to high-day errors\n",
    "- Wired W&B into the `train()` function via the optional `wandb_run=` parameter\n",
    "- Trained 6 runs (3 variants × 2 tasks) and logged every epoch's loss to W&B\n",
    "- Compared final test metrics in the W&B dashboard and locally\n",
    "\n",
    "**What the W&B dashboard gives you:**\n",
    "- Live loss curves for every run (find it at wandb.ai → project 'screenmind')\n",
    "- A sortable table of all runs with their hyperparameters and final metrics\n",
    "- Easy filtering: e.g. \"show me all runs with AUC > 0.85\"\n",
    "\n",
    "**Next: Milestone 6 — RAG Knowledge Base**  \n",
    "We will embed scientific papers into ChromaDB and build a retriever that fetches relevant context\n",
    "given a patient's risk factors. This becomes the knowledge source for the LangGraph agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ro2gom34vfj",
   "metadata": {},
   "source": [
    "## Bonus: Regression on the \"Affected\" Subpopulation (≥1 bad day)\n",
    "\n",
    "### The zero-inflation problem\n",
    "\n",
    "60% of BRFSS respondents report **0** bad mental health days.  When the\n",
    "regression model trains on the full dataset, the loss is dominated by this\n",
    "zero-inflated majority and the model learns to predict low values for almost everyone.\n",
    "\n",
    "A different, arguably more actionable question is:\n",
    "\n",
    "> **Among people who already have *some* mental health burden, how severe is it?**\n",
    "\n",
    "We answer this by filtering to rows where `y_reg > 0` and retraining.\n",
    "\n",
    "**What changes:**\n",
    "- The task shifts from \"predict burden for any US adult\" to \"predict severity among affected adults\"\n",
    "- Training set shrinks from ~315k → ~125k rows\n",
    "- Mean target rises from ~4.4 days to ~11 days\n",
    "- R² and MAE should be interpreted relative to this subgroup, not the full population\n",
    "- This is a legitimate modelling choice — many clinical tools focus on *severity among the affected*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "l1ot00mnm3i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset sizes (y_reg > 0):\n",
      "  Full train:      314,659  →  Affected: 126,006  (40.0%)\n",
      "  Full val:         67,427  →  Affected:  26,830  (39.8%)\n",
      "  Full test:        67,428  →  Affected:  26,769  (39.7%)\n",
      "\n",
      "  Mean MENTHLTH (full population):    4.42 days\n",
      "  Mean MENTHLTH (affected subset):    11.04 days\n",
      "  Baseline MAE on subset (predict mean): 8.569\n",
      "Training on cpu  |  max_epochs=100  |  patience=10\n",
      " Epoch    Train Loss      Val Loss    Best\n",
      "--------------------------------------------\n",
      "     1     132.09844      81.41433   ✓\n",
      "     2      79.70052      75.26604   ✓\n",
      "     3      78.28926      74.93330   ✓\n",
      "     4      77.92303      74.82152   ✓\n",
      "     5      77.72394      74.73298   ✓\n",
      "     6      77.63907      74.76095  \n",
      "     7      77.48473      74.60441   ✓\n",
      "     8      77.38590      74.78923  \n",
      "     9      77.32826      74.69868  \n",
      "    10      77.25120      74.59886   ✓\n",
      "    11      77.09087      74.69891  \n",
      "    12      77.03165      74.56622   ✓\n",
      "    13      77.06529      74.70201  \n",
      "    14      76.84876      74.51564   ✓\n",
      "    15      76.81003      74.47257   ✓\n",
      "    16      76.86721      74.49294  \n",
      "    17      76.75847      74.52438  \n",
      "    18      76.82428      74.55176  \n",
      "    19      76.70098      74.61228  \n",
      "    20      76.72454      74.64300  \n",
      "    21      76.64276      74.56980  \n",
      "    22      76.69780      74.47233   ✓\n",
      "    23      76.58591      74.52432  \n",
      "    24      76.53987      74.45867   ✓\n",
      "    25      76.38718      74.56608  \n",
      "    26      76.44474      74.50924  \n",
      "    27      76.37861      74.46527  \n",
      "    28      76.36824      74.43352   ✓\n",
      "    29      76.25056      74.41000   ✓\n",
      "    30      76.39991      74.39322   ✓\n",
      "    31      76.27951      74.41628  \n",
      "    32      76.19867      74.37865   ✓\n",
      "    33      76.20553      74.34361   ✓\n",
      "    34      76.16486      74.40896  \n",
      "    35      76.17966      74.39255  \n",
      "    36      76.10090      74.28619   ✓\n",
      "    37      76.07175      74.29746  \n",
      "    38      76.08268      74.38834  \n",
      "    39      76.09816      74.42896  \n",
      "    40      76.04598      74.34215  \n",
      "    41      75.93654      74.21140   ✓\n",
      "    42      75.99056      74.42236  \n",
      "    43      75.98231      74.35900  \n",
      "    44      75.89702      74.25078  \n",
      "    45      75.95201      74.30050  \n",
      "    46      75.82881      74.30547  \n",
      "    47      75.96685      74.19878   ✓\n",
      "    48      75.91708      74.33345  \n",
      "    49      75.75698      74.25340  \n",
      "    50      75.77265      74.26732  \n",
      "    51      75.82530      74.25785  \n",
      "    52      75.75900      74.53547  \n",
      "    53      75.83602      74.23474  \n",
      "    54      75.75020      74.29692  \n",
      "    55      75.74903      74.39277  \n",
      "    56      75.68387      74.30010  \n",
      "    57      75.53795      74.19160   ✓\n",
      "    58      75.68840      74.25258  \n",
      "    59      75.54869      74.25597  \n",
      "    60      75.50067      74.29597  \n",
      "    61      75.61928      74.26834  \n",
      "    62      75.54940      74.20232  \n",
      "    63      75.67180      74.27287  \n",
      "    64      75.58829      74.17708   ✓\n",
      "    65      75.63229      74.20062  \n",
      "    66      75.54464      74.16046   ✓\n",
      "    67      75.60674      74.24625  \n",
      "    68      75.52745      74.27959  \n",
      "    69      75.45225      74.15425   ✓\n",
      "    70      75.53578      74.25578  \n",
      "    71      75.40513      74.25228  \n",
      "    72      75.36538      74.19754  \n",
      "    73      75.35784      74.22119  \n",
      "    74      75.43516      74.13285   ✓\n",
      "    75      75.51090      74.13873  \n",
      "    76      75.34325      74.13292  \n",
      "    77      75.40119      74.25573  \n",
      "    78      75.40828      74.21422  \n",
      "    79      75.36093      74.23599  \n",
      "    80      75.30656      74.18330  \n",
      "    81      75.32227      74.20607  \n",
      "    82      75.33943      74.05912   ✓\n",
      "    83      75.49569      74.09203  \n",
      "    84      75.42099      74.18367  \n",
      "    85      75.33952      74.09678  \n",
      "    86      75.25747      74.23374  \n",
      "    87      75.31445      74.09255  \n",
      "    88      75.28369      74.16133  \n",
      "    89      75.30994      74.15029  \n",
      "    90      75.25967      74.09677  \n",
      "    91      75.23398      74.23522  \n",
      "    92      75.26571      74.10117  \n",
      "\n",
      "Early stopping at epoch 92 (no improvement for 10 epochs)\n",
      "\n",
      "Best val loss: 74.05912  →  saved to ..\\data\\models\\reg_affected_best.pt\n",
      "\n",
      "=== Affected Subset Regressor (y_reg > 0) ===\n",
      "  MAE:  6.805 days\n",
      "  RMSE: 8.617 days\n",
      "  R²:   0.2693\n",
      "\n",
      "=== Comparison ===\n",
      "  Full-population model  — MAE: 4.155  R²: 0.3521\n",
      "  Affected-only model    — MAE: 6.805  R²: 0.2693\n",
      "\n",
      "Note: the two MAEs are NOT directly comparable — they measure error\n",
      "on different test sets (full vs affected subset) with different mean targets.\n",
      "R² is more informative: it measures how much variance within each group is explained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# ── 1. Filter each split to rows where y_reg > 0 ──────────────────────────────\n",
    "mask_train = data['y_reg_train'] > 0\n",
    "mask_val   = data['y_reg_val']   > 0\n",
    "mask_test  = data['y_reg_test']  > 0\n",
    "\n",
    "X_train_aff = data['X_train'][mask_train]\n",
    "y_train_aff = data['y_reg_train'][mask_train]\n",
    "X_val_aff   = data['X_val'][mask_val]\n",
    "y_val_aff   = data['y_reg_val'][mask_val]\n",
    "X_test_aff  = data['X_test'][mask_test]\n",
    "y_test_aff  = data['y_reg_test'][mask_test]\n",
    "\n",
    "print('Subset sizes (y_reg > 0):')\n",
    "print(f'  Full train:     {len(data[\"X_train\"]):>8,}  →  Affected: {len(X_train_aff):>7,}  ({len(X_train_aff)/len(data[\"X_train\"])*100:.1f}%)')\n",
    "print(f'  Full val:       {len(data[\"X_val\"]):>8,}  →  Affected: {len(X_val_aff):>7,}  ({len(X_val_aff)/len(data[\"X_val\"])*100:.1f}%)')\n",
    "print(f'  Full test:      {len(data[\"X_test\"]):>8,}  →  Affected: {len(X_test_aff):>7,}  ({len(X_test_aff)/len(data[\"X_test\"])*100:.1f}%)')\n",
    "print()\n",
    "print(f'  Mean MENTHLTH (full population):    {data[\"y_reg_train\"].mean():.2f} days')\n",
    "print(f'  Mean MENTHLTH (affected subset):    {y_train_aff.mean():.2f} days')\n",
    "print(f'  Baseline MAE on subset (predict mean): {abs(y_test_aff - y_test_aff.mean()).mean():.3f}')\n",
    "\n",
    "# ── 2. Build DataLoaders manually ─────────────────────────────────────────────\n",
    "from torch.utils.data import DataLoader\n",
    "from src.training.trainer import BRFSSDataset\n",
    "\n",
    "train_loader_aff = DataLoader(BRFSSDataset(X_train_aff, y_train_aff), batch_size=512, shuffle=True,  num_workers=0)\n",
    "val_loader_aff   = DataLoader(BRFSSDataset(X_val_aff,   y_val_aff),   batch_size=512, shuffle=False, num_workers=0)\n",
    "test_loader_aff  = DataLoader(BRFSSDataset(X_test_aff,  y_test_aff),  batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "# ── 3. Train ──────────────────────────────────────────────────────────────────\n",
    "model_aff = MLP(input_dim=15, hidden_dims=[128, 64], dropout=0.3, task='reg')\n",
    "criterion_aff = make_criterion('reg', y_train_aff)\n",
    "\n",
    "history_aff = train(\n",
    "    model=model_aff,\n",
    "    train_loader=train_loader_aff,\n",
    "    val_loader=val_loader_aff,\n",
    "    criterion=criterion_aff,\n",
    "    checkpoint_path=f'{MODELS_DIR}/reg_affected_best.pt',\n",
    "    lr=1e-3,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "# ── 4. Evaluate ───────────────────────────────────────────────────────────────\n",
    "model_aff.load_state_dict(torch.load(f'{MODELS_DIR}/reg_affected_best.pt', weights_only=True))\n",
    "metrics_aff = evaluate_reg(model_aff, test_loader_aff)\n",
    "\n",
    "# Pull baseline reg metrics from the all_results list defined in cell 5\n",
    "# (metrics_reg lives in 03_model.ipynb, not here)\n",
    "baseline_reg = next(r for r in all_results if r['run'] == 'variant_a_baseline_reg')\n",
    "\n",
    "print()\n",
    "print('=== Affected Subset Regressor (y_reg > 0) ===')\n",
    "print(f'  MAE:  {metrics_aff[\"mae\"]:.3f} days')\n",
    "print(f'  RMSE: {metrics_aff[\"rmse\"]:.3f} days')\n",
    "print(f'  R²:   {metrics_aff[\"r2\"]:.4f}')\n",
    "print()\n",
    "print('=== Comparison ===')\n",
    "print(f'  Full-population model  — MAE: {baseline_reg[\"mae\"]:.3f}  R²: {baseline_reg[\"r2\"]:.4f}')\n",
    "print(f'  Affected-only model    — MAE: {metrics_aff[\"mae\"]:.3f}  R²: {metrics_aff[\"r2\"]:.4f}')\n",
    "print()\n",
    "print('Note: the two MAEs are NOT directly comparable — they measure error')\n",
    "print('on different test sets (full vs affected subset) with different mean targets.')\n",
    "print('R² is more informative: it measures how much variance within each group is explained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be0e7a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 6.804905891418457,\n",
       " 'rmse': 8.617044068955904,\n",
       " 'r2': 0.26925361156463623,\n",
       " 'n_samples': 26769}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88590e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 'variant_a_baseline_reg',\n",
       " 'task': 'reg',\n",
       " 'mae': 4.155448913574219,\n",
       " 'rmse': 6.720008625751591,\n",
       " 'r2': 0.35211753845214844,\n",
       " 'n_samples': 67428}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
